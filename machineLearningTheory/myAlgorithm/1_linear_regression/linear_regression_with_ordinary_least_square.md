# Linear regression with ordinary least square method

In simple linear regression, the relationship between the independent variable (X) and the dependent variable (Y) is given by following equation:

$Y=\theta_{1}X+\theta_{0}$

## Residual sum of squares (RSS)

In order to fit the best intercept line between the points in the above scatter plots, we use a metric called "Residual Sum of Squared" Errors and compare the lines to find out the best fit by reducing errors. The errors are sum difference between actual value and predicted value.

To find the errors for each dependent value, we need to use the formula below.

$RSS=\displaystyle\sum _{i=1}^{n} (y_{i}-\hat y_{i})^{2}$

$RSS=\displaystyle\sum _{i=1}^{n} (y_{i}-\theta_{1}x_{i}-\theta_{0})^{2}$
For your information: The **Mean Squared Error** is the average of the Residual sum of squares (RSS)

$MSE = \frac{1}{n}\displaystyle\sum_{i=1}^{n} (y_{i}-\hat{y_i})^2$

## Solution

After a bit of calculus and derivatives (see demo here: https://convert-calculate.com/math/linear-regression-using-ordinary-least-squares-calculator/)

### Slope

Slope: $m=\frac{\sum_{i=1}^{n}(x_i-\bar{X})(y_i-\bar{Y})}{\sum_{i=1}^{n}(x_i-\bar{X})^2}$

### Intercept

$b = \bar{Y} â€“ m\bar{X}$

